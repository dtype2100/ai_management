{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120d246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading: koalpaca from beomi/KoAlpaca-v1.1a\n",
      "âœ… koalpaca usable: 21155\n",
      "ğŸ”„ Loading: openorca from kyujinpy/KOR-OpenOrca-Platypus-v3\n",
      "âœ… openorca usable: 34212\n",
      "ğŸ”„ Loading: kullm from nlpai-lab/kullm-v2\n",
      "âœ… kullm usable: 151346\n",
      "ğŸ”„ Loading: sharegpt from FreedomIntelligence/sharegpt-korean\n",
      "âœ… sharegpt usable: 6011\n",
      "âœ… ìµœì¢… ì €ì¥ ì™„ë£Œ: ..\\data\\merged_ko_instruction_100k.jsonl (ì´ ìƒ˜í”Œ ìˆ˜: 100000)\n"
     ]
    }
   ],
   "source": [
    "# 100 K ìƒ˜í”Œë¡œ ë¬´ì‘ìœ„ ì¶”ì¶œ\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# ğŸ”§ ì‚¬ìš©í•  í•œêµ­ì–´ ë°ì´í„°ì…‹\n",
    "datasets_config = {\n",
    "    \"koalpaca\": {\n",
    "        \"path\": \"beomi/KoAlpaca-v1.1a\",\n",
    "        \"split\": \"train\"\n",
    "    },\n",
    "    \"openorca\": {\n",
    "        \"path\": \"kyujinpy/KOR-OpenOrca-Platypus-v3\",\n",
    "        \"split\": \"train\"\n",
    "    },\n",
    "    \"kullm\": {\n",
    "        \"path\": \"nlpai-lab/kullm-v2\",\n",
    "        \"split\": \"train\"\n",
    "    },\n",
    "    \"sharegpt\": {\n",
    "        \"path\": \"FreedomIntelligence/sharegpt-korean\",\n",
    "        \"split\": \"train\"\n",
    "    }\n",
    "}\n",
    "\n",
    "merged_data = []\n",
    "all_valid_entries = []\n",
    "\n",
    "# âœ… ë³€í™˜ í•¨ìˆ˜ (KoAlpaca í¬ë§· í†µì¼)\n",
    "def convert_to_koalpaca_format(name, entry):\n",
    "    if name == \"sharegpt\":\n",
    "        conversations = entry.get(\"conversations\", [])\n",
    "        if len(conversations) >= 2:\n",
    "            return {\n",
    "                \"instruction\": conversations[0].get(\"value\", \"\").strip(),\n",
    "                \"input\": \"\",\n",
    "                \"output\": conversations[1].get(\"value\", \"\").strip()\n",
    "            }\n",
    "    else:\n",
    "        instruction = entry.get(\"instruction\") or entry.get(\"prompt\", \"\")\n",
    "        output = entry.get(\"output\") or entry.get(\"completion\", \"\")\n",
    "        if instruction and output:\n",
    "            return {\n",
    "                \"instruction\": instruction.strip(),\n",
    "                \"input\": \"\",\n",
    "                \"output\": output.strip()\n",
    "            }\n",
    "    return None\n",
    "\n",
    "# âœ… ëª¨ë“  ìœ íš¨ ë°ì´í„° ëª¨ìœ¼ê¸°\n",
    "for name, config in datasets_config.items():\n",
    "    print(f\"ğŸ”„ Loading: {name} from {config['path']}\")\n",
    "    dataset = load_dataset(config[\"path\"], split=config[\"split\"])\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "    valid = []\n",
    "    for entry in dataset:\n",
    "        item = convert_to_koalpaca_format(name, entry)\n",
    "        if item:\n",
    "            valid.append(item)\n",
    "\n",
    "    print(f\"âœ… {name} usable: {len(valid)}\")\n",
    "    all_valid_entries.extend(valid)\n",
    "\n",
    "# âœ… 100Kë¡œ ë¬´ì‘ìœ„ ì¶”ì¶œ\n",
    "random.seed(42)\n",
    "final_dataset = random.sample(all_valid_entries, min(100_000, len(all_valid_entries)))\n",
    "\n",
    "# ğŸ’¾ ì €ì¥\n",
    "output_path = Path(\"../data/merged_ko_instruction_100k.jsonl\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in final_dataset:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… ìµœì¢… ì €ì¥ ì™„ë£Œ: {output_path} (ì´ ìƒ˜í”Œ ìˆ˜: {len(final_dataset)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a537ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ íŒŒì¼ ìš©ëŸ‰: 154.54 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"../data/merged_ko_instruction_100k.jsonl\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    print(f\"ğŸ“¦ íŒŒì¼ ìš©ëŸ‰: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7fc8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: 100000\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/merged_ko_instruction_100k.jsonl\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    line_count = sum(1 for _ in f)\n",
    "\n",
    "print(f\"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {line_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac039b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51efd64319414b419ac1575b368d413a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jsonl_path = \"../data/merged_ko_instruction_100k.jsonl\"\n",
    "raw_dataset = load_dataset(\"json\", data_files=jsonl_path, split=\"train\")\n",
    "split_dataset = raw_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_raw = split_dataset[\"train\"]\n",
    "val_raw = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ab678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892deed97efd439a9447971d355db029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f623e52044ef44f493169895cab8f028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/95 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f8f97f91fe46128ee8c9c9eaeffbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b59ebda6c6455b968a8e9a2fb00866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jwlee-ai/ko-instruct-100k-merged/commit/79217bd31e6cc0ed3c40635e8722579a606f23fc', commit_message='Upload dataset', commit_description='', oid='79217bd31e6cc0ed3c40635e8722579a606f23fc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jwlee-ai/ko-instruct-100k-merged', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jwlee-ai/ko-instruct-100k-merged'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 2. ë¡œì»¬ JSONL íŒŒì¼ ë¡œë“œ ë° ë¶„í• \n",
    "jsonl_path = \"../data/merged_ko_instruction_100k.jsonl\"\n",
    "raw_dataset = load_dataset(\"json\", data_files=jsonl_path, split=\"train\")\n",
    "split_dataset = raw_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "# 3. DatasetDictë¡œ ë¬¶ê¸°\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"validation\": split_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "# 4. ë°ì´í„°ì…‹ í‘¸ì‹œ (dataset repoê°€ ì—†ë‹¤ë©´ ìë™ ìƒì„±ë¨)\n",
    "dataset_dict.push_to_hub(\"jwlee-ai/ko-instruct-mix-100k\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
