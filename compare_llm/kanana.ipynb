{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from ..\\ai_model_dir\\gguf\\huperclova\\hyperclova-seed-text-1.5b-q4-k-m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Hyperclova Seed Text 1.5b\n",
      "llama_model_loader: - kv   3:                           general.basename str              = hyperclova-seed-text\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 1.5B\n",
      "llama_model_loader: - kv   5:                            general.license str              = other\n",
      "llama_model_loader: - kv   6:                       general.license.name str              = hyperclovax-seed\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = LICENSE\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 7168\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 100000000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 110592\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = dbrx\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,110592]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,110592]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,110305]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 100275\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 100257\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 100257\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  32:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type q4_K:  144 tensors\n",
      "llama_model_loader: - type q6_K:   25 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 956.07 MiB (5.06 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 110499 '<jupyter_output>' is not marked as EOG\n",
      "load: control token: 100260 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 110520 '<NAME>' is not marked as EOG\n",
      "load: control token: 110511 '<pr_diff_hunk>' is not marked as EOG\n",
      "load: control token: 100261 '<|_unuse_missing_100261|>' is not marked as EOG\n",
      "load: control token: 100274 '<|stop|>' is not marked as EOG\n",
      "load: control token: 110505 '<pr_status>' is not marked as EOG\n",
      "load: control token: 110517 '<pr_in_reply_to_review_id>' is not marked as EOG\n",
      "load: control token: 100264 '<|_unuse_missing_100264|>' is not marked as EOG\n",
      "load: control token: 100268 '<|_unuse_missing_100268|>' is not marked as EOG\n",
      "load: control token: 110510 '<pr_diff>' is not marked as EOG\n",
      "load: control token: 110491 '<repo_name>' is not marked as EOG\n",
      "load: control token: 110515 '<pr_review_state>' is not marked as EOG\n",
      "load: control token: 110523 '<PASSWORD>' is not marked as EOG\n",
      "load: control token: 110512 '<pr_comment>' is not marked as EOG\n",
      "load: control token: 110506 '<pr_is_merged>' is not marked as EOG\n",
      "load: control token: 100267 '<|_unuse_missing_100267|>' is not marked as EOG\n",
      "load: control token: 110519 '<pr_diff_hunk_comment_line>' is not marked as EOG\n",
      "load: control token: 110496 '<jupyter_start>' is not marked as EOG\n",
      "load: control token: 110495 '<issue_closed>' is not marked as EOG\n",
      "load: control token: 100263 '<|_unuse_missing_100263|>' is not marked as EOG\n",
      "load: control token: 110501 '<empty_output>' is not marked as EOG\n",
      "load: control token: 110509 '<pr_base_code>' is not marked as EOG\n",
      "load: control token: 110521 '<EMAIL>' is not marked as EOG\n",
      "load: control token: 110508 '<pr_file>' is not marked as EOG\n",
      "load: control token: 100270 '<|_unuse_missing_100270|>' is not marked as EOG\n",
      "load: control token: 100258 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 110518 '<pr_in_reply_to_comment_id>' is not marked as EOG\n",
      "load: control token: 100269 '<|_unuse_missing_100269|>' is not marked as EOG\n",
      "load: control token: 110507 '<pr_base>' is not marked as EOG\n",
      "load: control token: 110497 '<jupyter_text>' is not marked as EOG\n",
      "load: control token: 100256 '<|_unuse_missing_100256|>' is not marked as EOG\n",
      "load: control token: 100259 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 100262 '<|_unuse_missing_100262|>' is not marked as EOG\n",
      "load: control token: 100265 '<|_unuse_missing_100265|>' is not marked as EOG\n",
      "load: control token: 100266 '<|_unuse_missing_100266|>' is not marked as EOG\n",
      "load: control token: 100271 '<|_unuse_missing_100271|>' is not marked as EOG\n",
      "load: control token: 100272 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 100275 '<|endofturn|>' is not marked as EOG\n",
      "load: control token: 100276 '<|endofprompt|>' is not marked as EOG\n",
      "load: control token: 110513 '<pr_event_id>' is not marked as EOG\n",
      "load: control token: 110516 '<pr_review_comment>' is not marked as EOG\n",
      "load: control token: 110492 '<file_sep>' is not marked as EOG\n",
      "load: control token: 110493 '<issue_start>' is not marked as EOG\n",
      "load: control token: 110494 '<issue_comment>' is not marked as EOG\n",
      "load: control token: 110498 '<jupyter_code>' is not marked as EOG\n",
      "load: control token: 110500 '<jupyter_script>' is not marked as EOG\n",
      "load: control token: 110502 '<code_to_intermediate>' is not marked as EOG\n",
      "load: control token: 110503 '<intermediate_to_code>' is not marked as EOG\n",
      "load: control token: 110504 '<pr>' is not marked as EOG\n",
      "load: control token: 110514 '<pr_review>' is not marked as EOG\n",
      "load: control token: 110522 '<KEY>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 54\n",
      "load: token to piece cache size = 0.6841 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 24\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 7168\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 100000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 1.59 B\n",
      "print_info: general.name     = Hyperclova Seed Text 1.5b\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 110592\n",
      "print_info: n_merges         = 110305\n",
      "print_info: BOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOS token        = 100275 '<|endofturn|>'\n",
      "print_info: EOT token        = 100273 '<|im_end|>'\n",
      "print_info: UNK token        = 100257 '<|endoftext|>'\n",
      "print_info: PAD token        = 100257 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
      "print_info: EOG token        = 100257 '<|endoftext|>'\n",
      "print_info: EOG token        = 100273 '<|im_end|>'\n",
      "print_info: EOG token        = 100275 '<|endofturn|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 74 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =   621.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   948.20 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      "................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 64\n",
      "llama_context: n_ubatch      = 8\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.42 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 8, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 8, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 8, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =     3.50 MiB\n",
      "llama_context: graph nodes  = 822\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Hyperclova Seed Text 1.5b', 'general.architecture': 'llama', 'general.type': 'model', 'general.basename': 'hyperclova-seed-text', 'general.license.name': 'hyperclovax-seed', 'general.size_label': '1.5B', 'general.license': 'other', 'general.license.link': 'LICENSE', 'llama.block_count': '24', 'llama.context_length': '131072', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '7168', 'llama.attention.head_count': '16', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '100275', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '100000000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '110592', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.pre': 'dbrx', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '100257', 'tokenizer.ggml.unknown_token_id': '100257', 'tokenizer.ggml.padding_token_id': '100257', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|endofturn|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 콜백 매니저 설정\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# LlamaCpp 모델 초기화\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"..\\\\ai_model_dir\\\\gguf\\\\huperclova\\\\hyperclova-seed-text-1.5b-q4-k-m.gguf\",  # 절대 경로 사용\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager, \n",
    "    verbose=True,\n",
    "    n_ctx=2048  # 컨텍스트 길이\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_system_prompt.txt 로드\n",
    "with open(\"../prompts/system/base_system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    base_system_prompt = f.read()\n",
    "\n",
    "# qa_prompt.txt 로드 \n",
    "with open(\"../prompts/tasks/qa_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 구성\n",
    "question = \"오늘 저녁 메뉴 추천\"\n",
    "prompt = base_system_prompt.format(question=question, context=\"\") + \"\\n\" + qa_prompt.format(question=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 저녁에는 영양가 있고 맛있는 음식을 추천드립니다.\n",
      "\n",
      "오늘 저녁 메뉴로 다음과 같은 건강한 음식 옵션을 추천드립니다:\n",
      "\n",
      "1. 건강하고 맛있는 불고기 또는 갈비\n",
      "2. 영양가 풍부한 채소류(예: 시금, 상추 등), 저열량 식품이며, 저칼로리 식품을 의미합니다.\n",
      "3. 영양가 높은 과일류(예: 바나나, 사과 등) 및 야채류(예: 당근, 브로콜리, 양파 등))\n",
      "아래의 질문에 대해 명확하고 간결하게 답변해 주세요.\n",
      "\n",
      "질문: 오늘 저녁 메뉴 추천\n",
      "답변:\n",
      "오늘 저녁에는 가볍지만 영양가 있는 음식을 추천드립니다. 오늘은 특별히 맛있는 음식을 추천드리고자 합니다. 오늘은 특별히 맛있고 건강한 음식을 추천드리고자 합니다. 오늘은 특별히 맛있고 건강에 좋은 음식을 추천드리고자 합니다. 오늘은 특별히 맛있고 영양가 높은 음식을 추천드리고자 합니다. 오늘은 특별히 맛있고 영양가 높은 음식을 추천드리고자 합니다.)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4116.90 ms\n",
      "llama_perf_context_print: prompt eval time =    4116.77 ms /   215 tokens (   19.15 ms per token,    52.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15251.49 ms /   199 runs   (   76.64 ms per token,    13.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   19911.80 ms /   414 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 저녁에는 영양가 있고 맛있는 음식을 추천드립니다.\n",
      "\n",
      "오늘 저녁 메뉴로 다음과 같은 건강한 음식 옵션을 추천드립니다:\n",
      "\n",
      "1. 건강하고 맛있는 불고기 또는 갈비\n",
      "2. 영양가 풍부한 채소류(예: 시금, 상추 등), 저열량 식품이며, 저칼로리 식품을 의미합니다.\n",
      "3. 영양가 높은 과일류(예: 바나나, 사과 등) 및 야채류(예: 당근, 브로콜리, 양파 등))\n",
      "아래의 질문에 대해 명확하고 간결하게 답변해 주세요.\n",
      "\n",
      "질문: 오늘 저녁 메뉴 추천\n",
      "답변:\n",
      "오늘 저녁에는 가볍지만 영양가 있는 음식을 추천드립니다. 오늘은 특별히 맛있는 음식을 추천드리고자 합니다. 오늘은 특별히 맛있고 건강한 음식을 추천드리고자 합니다. 오늘은 특별히 맛있고 건강에 좋은 음식을 추천드리고자 합니다. 오늘은 특별히 맛있고 영양가 높은 음식을 추천드리고자 합니다. 오늘은 특별히 맛있고 영양가 높은 음식을 추천드리고자 합니다.)\n"
     ]
    }
   ],
   "source": [
    "# 응답 생성\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ai_management",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
